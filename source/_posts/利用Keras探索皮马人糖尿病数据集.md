---
title: 利用Keras探索皮马人糖尿病数据集
copyright: true
date: 2018-05-24 12:51:09
tags:
    - 机器学习
    - Keras
categories:
    - 机器学习
    - Keras
---

## 目录
- [1.数据集介绍](#1.-数据集介绍)
- [2.导入数据集](#2.-导入数据集)
- [3.查看数据信息](#3.-查看数据信息)
- [4.使用Keras建立神经网络](#4.-使用Keras建立神经网络)
    - [4.1 定义模型](#4.1-定义模型)
- [5.测试神经网络](#5.-测试神经网络)  
    - [5.1 口算神经网络](#5.1-口算神经网络)
    - [5.2 分割数据](#5.2-分割数据)
        - [5.2.1 自动验证](#5.2.1-自动验证)
        - [5.2.2 手工验证](#5.2.2-手工验证)
        - [4.2.3 K折交叉验证](#5.2.3-手工K折交叉验证)
- [6.使用Scikit-Learn调用Keras的模型](#6.-使用Scikit-Learn调用Keras的模型)
    - [6.1 使用交叉验证检验深度学习模型](#6.1-使用交叉验证检验深度学习模型)
    - [6.2 使用网格搜索调整深度学习模型的参数](#6.2-使用网格搜索调整深度学习模型的参数)    


## 1. 数据集介绍
该数据集涵盖了皮马人的医疗记录，以及过去5年内是否有糖尿病，所有的数据都以数字的形式呈现。需要解决的问题是，判断一个instance是否有糖尿病（是为1否为0）。这显然是一个**二分类问题**。该数据集中有8个属性及1个类别，表示如下：

- 怀孕次数 --- Number of times pregnant
- 2小时口服葡萄糖耐量试验中的血浆葡萄糖浓度 --- Plasma glucose concentration a 2 hours in an oral glucose tolerance test
    舒张压（毫米汞柱）--- Diastolic blood pressure (mm Hg)
- 2小时血清胰岛素（mu U/ml) --- 2-Hour serum insulin (mu U/ml)
- 三头肌皮褶厚度 (毫米) --- Triceps skin fold thickness (mm)
- 体重指数（BMI）--- Body mass index (weight in kg/(height in m)^2)
- 糖尿病血系功能 --- Diabetes pedigree function
- 年龄（年）--- Age (years)
- 类别：过去5年内是否有糖尿病 --- Class variable (0 or 1)

## 2. 导入数据集


```python
import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import numpy as np
```


```python
data = pd.read_csv('../data/pima-indians-diabetes.csv', header=None)
```

## 3. 查看数据信息


```python
data.head()
```



```python
data.describe()
```


```python
data.shape
```
(768, 9)




```python
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 768 entries, 0 to 767
    Data columns (total 9 columns):
    0    768 non-null int64
    1    768 non-null int64
    2    768 non-null int64
    3    768 non-null int64
    4    768 non-null int64
    5    768 non-null float64
    6    768 non-null float64
    7    768 non-null int64
    8    768 non-null int64
    dtypes: float64(2), int64(7)
    memory usage: 54.1 KB


## 4. 使用Keras建立神经网络

### 4.1 定义模型
**Keras的模型由层构成**：我们建立一个Sequential模型，一层层加入神经元。第一步是确定输入层的数目正确：在创建模型时用input_dim参数确定。例如，有8个输入变量，就设成8。

隐层怎么设置？这个问题很难回答，需要慢慢试验。一般来说，如果网络够大，即使存在问题也不会有影响。这个例子里我们用3层全连接网络。

全连接层用Dense类定义：第一个参数是本层神经元个数，然后是初始化方式和激活函数。这里的初始化方法是0到0.05的连续型均匀分布（uniform），Keras的默认方法也是这个。也可以用高斯分布进行初始化（normal）。

前两层的激活函数是线性整流函数relu，最后一层的激活函数是S型函数sigmoid。之前大家喜欢用S型和正切函数，但现在线性整流函数效果更好。为了保证输出是0到1的概率数字，最后一层的激活函数是S型函数，这样映射到0.5的阈值函数也容易。前两个隐层分别有12和8个神经元，最后一层是1个神经元（是否有糖尿病）。


```python
from keras.models import Sequential
from keras.layers import Dense
```

使用随机梯度下降时最好固定随机数种子，这样你的代码每次运行的结果都一致。这种做法在演示结果、比较算法或debug时特别有效。你可以随便选种子：


```python
seed = 7
np.random.seed(seed)
```


```python
X=data.iloc[:,0:8]
Y=data.iloc[:,8]
```

开始创建模型


```python
model = Sequential()
model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))
model.add(Dense(8, init='uniform', activation='relu'))
model.add(Dense(1, init='normal', activation='sigmoid'))
```


```python
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, nb_epoch=150, batch_size=10)
```

    Epoch 1/150
    769/769 [==============================] - 0s 566us/step - loss: 0.2924 - acc: 0.8023
    Epoch 2/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2781 - acc: 0.8218
    Epoch 3/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2886 - acc: 0.7984
    Epoch 4/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2703 - acc: 0.8101
    Epoch 5/150
    769/769 [==============================] - 0s 121us/step - loss: 0.2691 - acc: 0.8153
    Epoch 6/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2794 - acc: 0.8075
    Epoch 7/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2739 - acc: 0.8036
    Epoch 8/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2842 - acc: 0.7958
    Epoch 9/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2643 - acc: 0.8101
    Epoch 10/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2796 - acc: 0.8036
    Epoch 11/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2654 - acc: 0.8036
    Epoch 12/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2708 - acc: 0.7984
    Epoch 13/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2655 - acc: 0.8088
    Epoch 14/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2715 - acc: 0.8088
    Epoch 15/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2717 - acc: 0.8114
    Epoch 16/150
    769/769 [==============================] - 0s 119us/step - loss: 0.2529 - acc: 0.8257
    Epoch 17/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2803 - acc: 0.7971
    Epoch 18/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2615 - acc: 0.8218
    Epoch 19/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2727 - acc: 0.8127
    Epoch 20/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2731 - acc: 0.8153
    Epoch 21/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2541 - acc: 0.8166
    Epoch 22/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2611 - acc: 0.8088
    Epoch 23/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2660 - acc: 0.8049
    Epoch 24/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2511 - acc: 0.8088
    Epoch 25/150
    769/769 [==============================] - 0s 130us/step - loss: 0.2729 - acc: 0.8075
    Epoch 26/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2649 - acc: 0.8153
    Epoch 27/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2603 - acc: 0.8062
    Epoch 28/150
    769/769 [==============================] - 0s 133us/step - loss: 0.2697 - acc: 0.7984
    Epoch 29/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2683 - acc: 0.8010
    Epoch 30/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2625 - acc: 0.8062
    Epoch 31/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2624 - acc: 0.8088
    Epoch 32/150
    769/769 [==============================] - 0s 130us/step - loss: 0.2614 - acc: 0.8127
    Epoch 33/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2676 - acc: 0.8075
    Epoch 34/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2850 - acc: 0.8010
    Epoch 35/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2495 - acc: 0.8127
    Epoch 36/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2582 - acc: 0.8101
    Epoch 37/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2615 - acc: 0.8140
    Epoch 38/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2530 - acc: 0.8101
    Epoch 39/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2635 - acc: 0.8270
    Epoch 40/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2671 - acc: 0.8062
    Epoch 41/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2652 - acc: 0.7971
    Epoch 42/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2699 - acc: 0.8075
    Epoch 43/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2646 - acc: 0.8101
    Epoch 44/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2658 - acc: 0.8179
    Epoch 45/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2672 - acc: 0.8088
    Epoch 46/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2629 - acc: 0.8075
    Epoch 47/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2534 - acc: 0.8101
    Epoch 48/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2640 - acc: 0.8036
    Epoch 49/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2570 - acc: 0.8101
    Epoch 50/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2592 - acc: 0.8101
    Epoch 51/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2565 - acc: 0.8114
    Epoch 52/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2687 - acc: 0.8049
    Epoch 53/150
    769/769 [==============================] - 0s 117us/step - loss: 0.2541 - acc: 0.8140
    Epoch 54/150
    769/769 [==============================] - 0s 134us/step - loss: 0.2517 - acc: 0.8166
    Epoch 55/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2533 - acc: 0.8192
    Epoch 56/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2515 - acc: 0.8231
    Epoch 57/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2634 - acc: 0.8088
    Epoch 58/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2573 - acc: 0.8114
    Epoch 59/150
    769/769 [==============================] - 0s 119us/step - loss: 0.2591 - acc: 0.8218
    Epoch 60/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2590 - acc: 0.8153
    Epoch 61/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2548 - acc: 0.8088
    Epoch 62/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2948 - acc: 0.8010
    Epoch 63/150
    769/769 [==============================] - 0s 131us/step - loss: 0.2717 - acc: 0.8075
    Epoch 64/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2694 - acc: 0.8153
    Epoch 65/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2523 - acc: 0.8114
    Epoch 66/150
    769/769 [==============================] - 0s 113us/step - loss: 0.2707 - acc: 0.8023
    Epoch 67/150
    769/769 [==============================] - 0s 114us/step - loss: 0.2647 - acc: 0.8153
    Epoch 68/150
    769/769 [==============================] - 0s 119us/step - loss: 0.2652 - acc: 0.7997
    Epoch 69/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2553 - acc: 0.8088
    Epoch 70/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2593 - acc: 0.8166
    Epoch 71/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2720 - acc: 0.8101
    Epoch 72/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2505 - acc: 0.8244
    Epoch 73/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2580 - acc: 0.8127
    Epoch 74/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2477 - acc: 0.8192
    Epoch 75/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2536 - acc: 0.8192
    Epoch 76/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2660 - acc: 0.8088
    Epoch 77/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2500 - acc: 0.8166
    Epoch 78/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2592 - acc: 0.8179
    Epoch 79/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2573 - acc: 0.8127
    Epoch 80/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2505 - acc: 0.8127
    Epoch 81/150
    769/769 [==============================] - 0s 138us/step - loss: 0.2544 - acc: 0.8101
    Epoch 82/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2623 - acc: 0.8088
    Epoch 83/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2553 - acc: 0.8257
    Epoch 84/150
    769/769 [==============================] - 0s 117us/step - loss: 0.2513 - acc: 0.8088
    Epoch 85/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2619 - acc: 0.8153
    Epoch 86/150
    769/769 [==============================] - 0s 116us/step - loss: 0.2533 - acc: 0.8166
    Epoch 87/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2700 - acc: 0.8036
    Epoch 88/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2525 - acc: 0.8231
    Epoch 89/150
    769/769 [==============================] - 0s 119us/step - loss: 0.2525 - acc: 0.8127
    Epoch 90/150
    769/769 [==============================] - 0s 115us/step - loss: 0.2619 - acc: 0.8075
    Epoch 91/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2507 - acc: 0.8153
    Epoch 92/150
    769/769 [==============================] - 0s 118us/step - loss: 0.2682 - acc: 0.8101
    Epoch 93/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2649 - acc: 0.8153
    Epoch 94/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2578 - acc: 0.8062
    Epoch 95/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2768 - acc: 0.8062
    Epoch 96/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2626 - acc: 0.8062
    Epoch 97/150
    769/769 [==============================] - 0s 131us/step - loss: 0.2588 - acc: 0.8023
    Epoch 98/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2653 - acc: 0.8062
    Epoch 99/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2629 - acc: 0.8127
    Epoch 100/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2610 - acc: 0.8205
    Epoch 101/150
    769/769 [==============================] - 0s 130us/step - loss: 0.2661 - acc: 0.8114
    Epoch 102/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2565 - acc: 0.8075
    Epoch 103/150
    769/769 [==============================] - 0s 130us/step - loss: 0.2617 - acc: 0.8114
    Epoch 104/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2590 - acc: 0.8205
    Epoch 105/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2590 - acc: 0.8166
    Epoch 106/150
    769/769 [==============================] - 0s 130us/step - loss: 0.2610 - acc: 0.8192
    Epoch 107/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2573 - acc: 0.8062
    Epoch 108/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2610 - acc: 0.8205
    Epoch 109/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2538 - acc: 0.8166
    Epoch 110/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2593 - acc: 0.8088
    Epoch 111/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2422 - acc: 0.8192
    Epoch 112/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2581 - acc: 0.8114
    Epoch 113/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2488 - acc: 0.8192
    Epoch 114/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2357 - acc: 0.8244
    Epoch 115/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2445 - acc: 0.8179
    Epoch 116/150
    769/769 [==============================] - 0s 128us/step - loss: 0.2425 - acc: 0.8166 0s - loss: 0.1024 - acc: 0.819
    Epoch 117/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2443 - acc: 0.8192
    Epoch 118/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2588 - acc: 0.8166
    Epoch 119/150
    769/769 [==============================] - 0s 117us/step - loss: 0.2568 - acc: 0.8075
    Epoch 120/150
    769/769 [==============================] - 0s 129us/step - loss: 0.2465 - acc: 0.8153
    Epoch 121/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2465 - acc: 0.8127
    Epoch 122/150
    769/769 [==============================] - 0s 132us/step - loss: 0.2434 - acc: 0.8231
    Epoch 123/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2535 - acc: 0.8127
    Epoch 124/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2630 - acc: 0.8101
    Epoch 125/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2554 - acc: 0.8166
    Epoch 126/150
    769/769 [==============================] - 0s 121us/step - loss: 0.2537 - acc: 0.8114
    Epoch 127/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2611 - acc: 0.8114
    Epoch 128/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2550 - acc: 0.8062
    Epoch 129/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2521 - acc: 0.8140
    Epoch 130/150
    769/769 [==============================] - 0s 119us/step - loss: 0.2669 - acc: 0.8010
    Epoch 131/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2624 - acc: 0.8010
    Epoch 132/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2465 - acc: 0.8218
    Epoch 133/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2493 - acc: 0.8101
    Epoch 134/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2461 - acc: 0.8179
    Epoch 135/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2435 - acc: 0.8218
    Epoch 136/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2564 - acc: 0.8023
    Epoch 137/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2505 - acc: 0.8179
    Epoch 138/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2517 - acc: 0.8166
    Epoch 139/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2549 - acc: 0.8127
    Epoch 140/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2648 - acc: 0.8049
    Epoch 141/150
    769/769 [==============================] - 0s 121us/step - loss: 0.2493 - acc: 0.8101
    Epoch 142/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2389 - acc: 0.8179
    Epoch 143/150
    769/769 [==============================] - 0s 126us/step - loss: 0.2513 - acc: 0.8192
    Epoch 144/150
    769/769 [==============================] - 0s 125us/step - loss: 0.2425 - acc: 0.8218
    Epoch 145/150
    769/769 [==============================] - 0s 120us/step - loss: 0.2484 - acc: 0.8205
    Epoch 146/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2465 - acc: 0.8205
    Epoch 147/150
    769/769 [==============================] - 0s 124us/step - loss: 0.2534 - acc: 0.8127
    Epoch 148/150
    769/769 [==============================] - 0s 122us/step - loss: 0.2484 - acc: 0.8270
    Epoch 149/150
    769/769 [==============================] - 0s 127us/step - loss: 0.2544 - acc: 0.8062
    Epoch 150/150
    769/769 [==============================] - 0s 123us/step - loss: 0.2572 - acc: 0.8205





    <keras.callbacks.History at 0x12328c210>



我们把测试数据拿出来检验一下模型的效果。注意这样不能测试在新数据的预测能力。应该将数据分成训练和测试集。

调用模型的evaluation()方法，传入训练时的数据。输出是平均值，包括平均误差和其他的数据，例如准确度。


```python
scores = model.evaluate(X,Y)
print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)
```

    769/769 [==============================] - 0s 75us/step
    acc: 82.575


## 5. 测试神经网络

深度学习有很多参数要调：大部分都是拍脑袋的。所以测试特别重要：本章我们讨论几种测试方法。本章将：

- 使用Keras进行自动验证
- 使用Keras进行手工验证
- 使用Keras进行K折交叉验证

### 5.1 口算神经网络
创建神经网络时有很多参数：很多时候可以从别人的网络上抄，但是最终还是需要一点点做实验。无论是网络的拓扑结构（层数、大小、每层类型）还是小参数（损失函数、激活函数、优化算法、训练次数）等。

一般深度学习的数据集都很大，数据有几十万乃至几亿个。所以测试方法至关重要。

### 5.2 分割数据
数据量大和网络复杂会造成训练时间很长，所以需要将数据分成训练、测试或验证数据集。Keras提供两种办法：

自动验证
手工验证
#### 5.2.1 自动验证
Keras可以将数据自动分出一部分，每次训练后进行验证。在训练时用**validation_split**参数可以指定验证数据的比例，一般是总数据的20%或者33%。


```python
model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10, verbose=False)
scores = model.evaluate(X,Y)
print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)
```

    769/769 [==============================] - 0s 17us/step
    acc: 81.274


#### 5.2.2 手工验证
Keras也可以手工进行验证。我们定义一个**train_test_split**函数，将数据分成2：1的测试和验证数据集。在调用fit()方法时需要加入**validation_data**参数作为验证数据，数组的项目分别是输入和输出数据。


```python
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, random_state=seed)
```


```python
model.fit(X_train, y_train, validation_data=(X_test,y_test), nb_epoch=150, batch_size=10, verbose=False)
scores = model.evaluate(X,Y)
print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)
```

    769/769 [==============================] - 0s 17us/step
    acc: 82.965


#### 5.2.3 手工K折交叉验证
机器学习的金科玉律是K折验证，以验证模型对未来数据的预测能力。K折验证的方法是：将数据分成K组，留下1组验证，其他数据用作训练，直到每种分发的性能一致。

深度学习一般不用交叉验证，因为对算力要求太高。例如，K折的次数一般是5或者10折：每组都需要训练并验证，训练时间成倍上升。然而，如果数据量小，交叉验证的效果更好，误差更小。

scikit-learn有StratifiedKFold类，我们用它把数据分成10组。抽样方法是分层抽样，尽可能保证每组数据量一致。然后我们在每组上训练模型，使用verbose=0参数关闭每轮的输出。训练后，Keras会输出模型的性能，并存储模型。最终，Keras输出性能的平均值和标准差，为性能估算提供更准确的估计。

**当然，这是种繁琐的做法。我们可以直接使用scikit-learn去调用keras完成K-fold validation.**

## 6. 使用Scikit-Learn调用Keras的模型
Keras为scikit-learn封装了KerasClassifier和KerasRegressor方便我们对模型进行调参。

### 6.1 使用交叉验证检验深度学习模型
Keras的KerasClassifier和KerasRegressor两个类接受build_fn参数，传入编译好的模型。我们加入nb_epoch=150和batch_size=10这两个参数：这两个参数会传入模型的fit()方法。我们用scikit-learn的StratifiedKFold类进行10折交叉验证，测试模型在未知数据的性能，并使用cross_val_score()函数检测模型，打印结果。


```python
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import cross_val_score


def create_model():
    model = Sequential()
    model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))
    model.add(Dense(8, init='uniform', activation='relu'))
    model.add(Dense(1,init='normal', activation='sigmoid'))

    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=['accuracy'])
    return model

X=data.iloc[:,0:8]
Y=data.iloc[:,8]

model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10)
kfold = StratifiedKFold(y=Y, n_folds=3, shuffle=True, random_state=seed)

results = cross_val_score(model, X, Y, cv=kfold)
print 'results:', results
print 'mean result:', results.mean()

```

    Epoch 1/1
    512/512 [==============================] - 1s 2ms/step - loss: 0.6788 - acc: 0.6504
    257/257 [==============================] - 0s 1ms/step
    Epoch 1/1
    512/512 [==============================] - 1s 2ms/step - loss: 0.6806 - acc: 0.6484
    257/257 [==============================] - 0s 1ms/step
    Epoch 1/1
    514/514 [==============================] - 1s 2ms/step - loss: 0.6768 - acc: 0.6420
    255/255 [==============================] - 0s 2ms/step
    results: [0.64980545 0.64980545 0.65098039]
    mean result: 0.6501970970798299


## 6.2 使用网格搜索调整深度学习模型的参数
使用scikit-learn封装Keras的模型十分简单。进一步想：我们可以给fit()方法传入参数，KerasClassifier的build_fn方法也可以传入参数。可以利用这点进一步调整模型。

我们用网格搜索测试不同参数的性能：create_model()函数可以传入optimizer和init参数，虽然都有默认值。那么我们可以用不同的优化算法和初始权重调整网络。具体说，我们希望搜索：

- 优化算法：搜索权重的方法
- 初始权重：初始化不同的网络
- 训练次数：对模型训练的次数
- 批次大小：每次训练的数据量
- 所有的参数组成一个字典，传入scikit-learn的GridSearchCV类：GridSearchCV会对每组参数（2×3×3×3）进行训练，进行3折交叉检验。

计算量巨大：耗时巨长。如果模型小还可以取一部分数据试试。比如我们这里使用的模型，网络和数据集都不大（1000个数据内，9个参数）。最后scikit-learn会输出最好的参数和模型，以及平均值。


```python
from sklearn.grid_search import GridSearchCV

def create_model(optimizer='rmsprop', init='glorot_uniform'):
    model = Sequential()
    model.add(Dense(12, input_dim=8, init=init, activation='relu'))
    model.add(Dense(8, init=init, activation='relu'))
    model.add(Dense(1, init=init, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model

X=data.iloc[:,0:8]
Y=data.iloc[:,8]   

model = KerasClassifier(build_fn=create_model)

optimizers = ['rmsprop', 'adam']
inits = ['glorot_uniform', 'uniform', 'normal']
epoches = [50, 100, 150]
batches = [5, 10, 20]

param_grid= dict(optimizer=optimizers, nb_epoch=epoches, batch_size=batches, init=inits)
grid = GridSearchCV(estimator=model, param_grid=param_grid)
results = grid.fit(X, Y)

print results


```

    Epoch 1/1
    512/512 [==============================] - 1s 2ms/step - loss: 2.1937 - acc: 0.6250
    257/257 [==============================] - 0s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 3.5229 - acc: 0.4366
    256/256 [==============================] - 0s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 2ms/step - loss: 1.6050 - acc: 0.5556
    256/256 [==============================] - 0s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 1s 3ms/step - loss: 1.7555 - acc: 0.5664
    257/257 [==============================] - 0s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 10.2795 - acc: 0.3528
    256/256 [==============================] - 0s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 3.6850 - acc: 0.5380
    256/256 [==============================] - 0s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 1s 3ms/step - loss: 5.3517 - acc: 0.6680
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 7.4982 - acc: 0.4386
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 1.7838 - acc: 0.5302
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 1s 3ms/step - loss: 6.7545 - acc: 0.4590
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 2.2323 - acc: 0.4815
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 2.4704 - acc: 0.5283
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 1s 3ms/step - loss: 3.4122 - acc: 0.6328
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 5.7050 - acc: 0.6452
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 5.4465 - acc: 0.6394
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 3ms/step - loss: 2.0756 - acc: 0.6191
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 3.7551 - acc: 0.5887
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 3.2473 - acc: 0.4230
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 3ms/step - loss: 0.6692 - acc: 0.6641
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6788 - acc: 0.6452
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6774 - acc: 0.6394
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 3ms/step - loss: 0.6799 - acc: 0.6484
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6855 - acc: 0.6433
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6785 - acc: 0.6374
    256/256 [==============================] - 1s 2ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 3ms/step - loss: 0.6670 - acc: 0.6719
    257/257 [==============================] - 1s 2ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6787 - acc: 0.6277
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 3ms/step - loss: 0.6898 - acc: 0.5965
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6774 - acc: 0.6387
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6818 - acc: 0.6452
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6755 - acc: 0.6374
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6700 - acc: 0.6621
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6783 - acc: 0.6413
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6789 - acc: 0.6257
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6840 - acc: 0.6230
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6850 - acc: 0.6452
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6894 - acc: 0.6394
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6808 - acc: 0.6641
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6788 - acc: 0.6238
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6781 - acc: 0.6335
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6726 - acc: 0.6328
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6767 - acc: 0.6452
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6867 - acc: 0.6062
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6671 - acc: 0.6680
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6765 - acc: 0.6277
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6690 - acc: 0.6374
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6716 - acc: 0.6426
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6793 - acc: 0.6218
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6808 - acc: 0.6374
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 0.6622 - acc: 0.6582
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6770 - acc: 0.6296
    256/256 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 0.6757 - acc: 0.6374
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 5ms/step - loss: 0.6807 - acc: 0.6602
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 0.6757 - acc: 0.6335
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 0.6753 - acc: 0.6218
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 4ms/step - loss: 5.3517 - acc: 0.6680
    257/257 [==============================] - 1s 3ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 4ms/step - loss: 5.6946 - acc: 0.6452
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 5.8061 - acc: 0.6374
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 5ms/step - loss: 10.6218 - acc: 0.3281
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 1.3061 - acc: 0.5867
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 4.6867 - acc: 0.4230
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 2s 5ms/step - loss: 2.8459 - acc: 0.5000
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 2.9081 - acc: 0.4561
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 2s 5ms/step - loss: 2.7068 - acc: 0.5965
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 5ms/step - loss: 5.3517 - acc: 0.6680
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 4.7872 - acc: 0.6413
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 4.8742 - acc: 0.3606
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 5ms/step - loss: 10.6189 - acc: 0.3320
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 1.9542 - acc: 0.5945
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 3.1041 - acc: 0.4191
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 5ms/step - loss: 2.6217 - acc: 0.6523
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 10.2764 - acc: 0.3528
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 2.5856 - acc: 0.6335
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 5ms/step - loss: 0.6789 - acc: 0.6680
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 0.6873 - acc: 0.6335
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 0.6773 - acc: 0.6296
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6812 - acc: 0.6699
    257/257 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 0.6866 - acc: 0.6238
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6847 - acc: 0.6335
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6711 - acc: 0.6660
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6804 - acc: 0.6355
    256/256 [==============================] - 1s 4ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 5ms/step - loss: 0.6852 - acc: 0.6238
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6809 - acc: 0.6602
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6913 - acc: 0.6023
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6894 - acc: 0.6043
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6811 - acc: 0.6582
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6806 - acc: 0.6452
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6824 - acc: 0.6374
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6667 - acc: 0.6680
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6829 - acc: 0.6452
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6915 - acc: 0.5244
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 6ms/step - loss: 0.6703 - acc: 0.6680
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6841 - acc: 0.6335
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6728 - acc: 0.6374
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 0.6825 - acc: 0.5977
    257/257 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 0.6882 - acc: 0.6277
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 0.6840 - acc: 0.6062
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 0.6655 - acc: 0.6660
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6905 - acc: 0.5595
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 6ms/step - loss: 0.6810 - acc: 0.6374
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 0.6667 - acc: 0.6367
    257/257 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 0.6731 - acc: 0.6472
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 0.6841 - acc: 0.6374
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 0.6777 - acc: 0.6387
    257/257 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 0.6844 - acc: 0.6277
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 0.6759 - acc: 0.6277
    256/256 [==============================] - 1s 5ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 0.6727 - acc: 0.6367
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 0.6818 - acc: 0.6433
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 0.6893 - acc: 0.6082
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 3s 7ms/step - loss: 5.3517 - acc: 0.6680
    257/257 [==============================] - 1s 5ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 3.2354 - acc: 0.3645
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 3s 7ms/step - loss: 4.6196 - acc: 0.5692
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 7ms/step - loss: 9.8583 - acc: 0.3320
    257/257 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 3.2941 - acc: 0.5497
    256/256 [==============================] - 1s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 7.3399 - acc: 0.3587
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 6s 11ms/step - loss: 2.0467 - acc: 0.6484
    257/257 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 10ms/step - loss: 2.9050 - acc: 0.5263
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 5.8311 - acc: 0.6374
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 8ms/step - loss: 3.9729 - acc: 0.3320
    257/257 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 11ms/step - loss: 5.1686 - acc: 0.4191
    256/256 [==============================] - 4s 14ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 5.0270 - acc: 0.5556
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 8ms/step - loss: 6.5165 - acc: 0.3398
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 6s 12ms/step - loss: 5.6138 - acc: 0.6452
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 6.2128 - acc: 0.5205
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 9ms/step - loss: 3.1858 - acc: 0.6328
    257/257 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 3.9268 - acc: 0.6140
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 4.6766 - acc: 0.5556
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 8ms/step - loss: 0.6850 - acc: 0.6426
    257/257 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6878 - acc: 0.6413
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 7ms/step - loss: 0.6865 - acc: 0.6277
    256/256 [==============================] - 2s 6ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 8ms/step - loss: 0.6846 - acc: 0.6309
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6878 - acc: 0.6296
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6908 - acc: 0.6160
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 5s 9ms/step - loss: 0.6757 - acc: 0.6680
    257/257 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6912 - acc: 0.5497
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 1s 3ms/step - loss: 0.6905 - acc: 0.6218
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 9ms/step - loss: 0.6914 - acc: 0.6230
    257/257 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6910 - acc: 0.5828
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6911 - acc: 0.5536
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 8ms/step - loss: 0.6828 - acc: 0.6445
    257/257 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6813 - acc: 0.6452
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6873 - acc: 0.6082
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 9ms/step - loss: 0.6848 - acc: 0.6582
    257/257 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 8ms/step - loss: 0.6892 - acc: 0.6101
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 9ms/step - loss: 0.6878 - acc: 0.6238
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 9ms/step - loss: 0.6905 - acc: 0.5879
    257/257 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 7s 13ms/step - loss: 0.6865 - acc: 0.6277
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 9ms/step - loss: 0.6851 - acc: 0.6374
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 5s 9ms/step - loss: 0.6762 - acc: 0.6680
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6781 - acc: 0.6394
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 10ms/step - loss: 0.6856 - acc: 0.6374
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 5s 9ms/step - loss: 0.6730 - acc: 0.6738
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 9ms/step - loss: 0.6822 - acc: 0.6277
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 9ms/step - loss: 0.6879 - acc: 0.5887
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 5s 9ms/step - loss: 0.6825 - acc: 0.6270
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 10ms/step - loss: 0.6854 - acc: 0.6199
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6875 - acc: 0.6374
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 4s 9ms/step - loss: 0.6697 - acc: 0.6680
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 4s 9ms/step - loss: 0.6866 - acc: 0.6062
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6875 - acc: 0.6004
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    512/512 [==============================] - 5s 9ms/step - loss: 0.6871 - acc: 0.5898
    257/257 [==============================] - 2s 8ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6817 - acc: 0.6257
    256/256 [==============================] - 2s 7ms/step
    Epoch 1/1
    513/513 [==============================] - 5s 9ms/step - loss: 0.6841 - acc: 0.6082
    256/256 [==============================] - 2s 8ms/step
    Epoch 1/1
    769/769 [==============================] - 5s 6ms/step - loss: 0.6814 - acc: 0.6502
    GridSearchCV(cv=None, error_score='raise',
           estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x134c56390>,
           fit_params={}, iid=True, n_jobs=1,
           param_grid={'init': ['glorot_uniform', 'uniform', 'normal'], 'optimizer': ['rmsprop', 'adam'], 'nb_epoch': [50, 100, 150], 'batch_size': [5, 10, 20]},
           pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)



```python
print("Best: %f using %s" % (results.best_score_, results.best_params_))
```

    Best: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}


通过上面的计算，我们得到了最佳模型的参数:
- Best: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}


```python
for params, mean_score, scores in results.grid_scores_:
    print("%f (%f) with: %r" % (scores.mean(), scores.std(), params))
```

    0.637165 (0.039419) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}
    0.488849 (0.105365) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}
    0.586439 (0.047494) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}
    0.529249 (0.024535) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}
    0.638527 (0.020554) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}
    0.520073 (0.045074) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}
    0.654147 (0.029988) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 5}
    0.650241 (0.025869) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}
    0.487785 (0.077297) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}
    0.574842 (0.037923) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}
    0.520033 (0.138485) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}
    0.538566 (0.111370) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}
    0.510898 (0.124085) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}
    0.652845 (0.028562) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 10}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 10}
    0.512220 (0.092497) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}
    0.478670 (0.110245) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}
    0.605940 (0.073768) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}
    0.503410 (0.086696) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}
    0.603503 (0.066787) with: {'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}
    0.563078 (0.076691) with: {'init': 'glorot_uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}
    0.654132 (0.020581) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'uniform', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 50, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 50, 'batch_size': 20}
    0.651543 (0.027187) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 100, 'batch_size': 20}
    0.659356 (0.031877) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'rmsprop', 'nb_epoch': 150, 'batch_size': 20}
    0.650241 (0.025869) with: {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 150, 'batch_size': 20}


```python
# st: 0.659298 using {'init': 'normal', 'optimizer': 'adam', 'nb_epoch': 100, 'batch_size': 20}
model = Sequential()
model.add(Dense(12, input_dim=8, init='normal', activation='relu'))
model.add(Dense(8, init='normal', activation='relu'))
model.add(Dense(1, init='normal', activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, random_state=seed)
model.fit(X_train, y_train, validation_data=(X_test,y_test), nb_epoch=100, batch_size=20, verbose=False)
scores = model.evaluate(X,Y)
print '%s: %.3f' % (model.metrics_names[1], scores[1]*100)
```

    769/769 [==============================] - 0s 67us/step
    acc: 72.432
